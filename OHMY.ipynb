{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991b596b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lichclass/Documents/GitHub/Animal_Re-Identification_Model/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXtBackbone(\n",
      "  (backbone): ConvNeXt(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "        )\n",
      "        (1): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.014285714285714285, mode=row)\n",
      "        )\n",
      "        (2): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.02857142857142857, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.04285714285714286, mode=row)\n",
      "        )\n",
      "        (1): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.05714285714285714, mode=row)\n",
      "        )\n",
      "        (2): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.07142857142857142, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.08571428571428572, mode=row)\n",
      "        )\n",
      "        (1): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "        )\n",
      "        (2): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.11428571428571428, mode=row)\n",
      "        )\n",
      "        (3): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.12857142857142856, mode=row)\n",
      "        )\n",
      "        (4): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.14285714285714285, mode=row)\n",
      "        )\n",
      "        (5): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.15714285714285714, mode=row)\n",
      "        )\n",
      "        (6): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.17142857142857143, mode=row)\n",
      "        )\n",
      "        (7): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.18571428571428572, mode=row)\n",
      "        )\n",
      "        (8): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
      "        )\n",
      "        (9): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.21428571428571427, mode=row)\n",
      "        )\n",
      "        (10): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.22857142857142856, mode=row)\n",
      "        )\n",
      "        (11): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.24285714285714285, mode=row)\n",
      "        )\n",
      "        (12): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.2571428571428571, mode=row)\n",
      "        )\n",
      "        (13): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.2714285714285714, mode=row)\n",
      "        )\n",
      "        (14): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.2857142857142857, mode=row)\n",
      "        )\n",
      "        (15): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.3, mode=row)\n",
      "        )\n",
      "        (16): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.3142857142857143, mode=row)\n",
      "        )\n",
      "        (17): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.32857142857142857, mode=row)\n",
      "        )\n",
      "        (18): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.34285714285714286, mode=row)\n",
      "        )\n",
      "        (19): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.35714285714285715, mode=row)\n",
      "        )\n",
      "        (20): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.37142857142857144, mode=row)\n",
      "        )\n",
      "        (21): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.38571428571428573, mode=row)\n",
      "        )\n",
      "        (22): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.4, mode=row)\n",
      "        )\n",
      "        (23): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.4142857142857143, mode=row)\n",
      "        )\n",
      "        (24): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.42857142857142855, mode=row)\n",
      "        )\n",
      "        (25): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.44285714285714284, mode=row)\n",
      "        )\n",
      "        (26): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.45714285714285713, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.4714285714285714, mode=row)\n",
      "        )\n",
      "        (1): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.4857142857142857, mode=row)\n",
      "        )\n",
      "        (2): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.5, mode=row)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (classifier): Sequential(\n",
      "      (0): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (proj): Linear(in_features=1024, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- PyTorch/Torchvision Imports ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from torch.amp import autocast_mode, grad_scaler\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import convnext_base, ConvNeXt_Base_Weights\n",
    "\n",
    "# --- Library Imports ---\n",
    "from wildlife_datasets.datasets import SeaTurtleID2022\n",
    "from wildlife_tools.data import ImageDataset\n",
    "from wildlife_datasets.splits import ClosedSetSplit\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = \"nashadammuoz\"\n",
    "os.environ['KAGGLE_KEY'] = \"KGAT_9f227e36a409b0debe5ee7a27090bd72\"\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "class ConvNeXtBackbone(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, dropout=0.2, pretrained=True):\n",
    "        super().__init__()\n",
    "        weights = ConvNeXt_Base_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        model = convnext_base(weights=weights)\n",
    "        # Remove original classifier\n",
    "        in_features = model.classifier[2].in_features\n",
    "        model.classifier[2] = nn.Identity()\n",
    "        self.backbone = model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(in_features, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        feat = self.dropout(feat)\n",
    "        emb = self.proj(feat)\n",
    "        # Return normalized embedding and norms (for AdaFace)\n",
    "        norms = torch.norm(emb, p=2, dim=1, keepdim=True)\n",
    "        emb = F.normalize(emb, dim=1)\n",
    "        return emb, norms.squeeze()\n",
    "\n",
    "\n",
    "class AdaFaceHead(nn.Module):\n",
    "    def __init__(self, embedding_size, num_classes, m=0.35, h=0.2, s=64., t_alpha=0.01):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.kernel = nn.Parameter(torch.Tensor(embedding_size, num_classes))\n",
    "        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n",
    "        self.m = m\n",
    "        self.h = h\n",
    "        self.s = s\n",
    "        self.t_alpha = t_alpha\n",
    "        self.register_buffer('batch_mean', torch.ones(1)*20)\n",
    "        self.register_buffer('batch_std', torch.ones(1)*100)\n",
    "\n",
    "    def forward(self, embeddings, norms, label):\n",
    "        kernel_norm = torch.nn.functional.normalize(self.kernel, dim=0)\n",
    "        cosine = torch.mm(embeddings, kernel_norm).clamp(-1+1e-3, 1-1e-3)\n",
    "\n",
    "        if label is None:\n",
    "            return cosine * self.s\n",
    "\n",
    "        with torch.no_grad():\n",
    "            std = norms.std() if norms.size(0) > 1 else torch.tensor(0.0, device=norms.device) # Handling for batch size 1\n",
    "            self.batch_mean = norms.mean() * self.t_alpha + (1 - self.t_alpha) * self.batch_mean\n",
    "            self.batch_std = std * self.t_alpha + (1 - self.t_alpha) * self.batch_std\n",
    "\n",
    "        margin_scaler = (norms - self.batch_mean) / (self.batch_std + 1e-3)\n",
    "        margin_scaler = torch.clip(margin_scaler * self.h, -1, 1)\n",
    "\n",
    "        # AdaFace logic\n",
    "        m_arc = torch.zeros_like(cosine)\n",
    "        m_arc.scatter_(1, label.view(-1, 1), 1.0)\n",
    "        g_angular = -self.m * margin_scaler\n",
    "        m_arc = m_arc * g_angular.unsqueeze(1)\n",
    "        \n",
    "        theta = cosine.acos()\n",
    "        theta_m = torch.clip(theta + m_arc, min=1e-3, max=math.pi-1e-3)\n",
    "        cosine_m = theta_m.cos()\n",
    "\n",
    "        m_cos = torch.zeros_like(cosine)\n",
    "        m_cos.scatter_(1, label.view(-1, 1), 1.0)\n",
    "        g_add = self.m + (self.m * margin_scaler)\n",
    "        m_cos = m_cos * g_add.unsqueeze(1)\n",
    "        \n",
    "        return (cosine_m - m_cos) * self.s\n",
    "\n",
    "\n",
    "# Wrapper Model\n",
    "class ReIDModel(nn.Module): \n",
    "    def __init__(self, backbone, head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        emb, norms = self.backbone(x)\n",
    "        if labels is not None:\n",
    "            logits = self.head(emb, norms, labels)\n",
    "            return logits, emb\n",
    "        return emb\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def extract_features(model, dataset, device, batch_size=16):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in tqdm(loader, desc=\"Extracting features\", leave=False):\n",
    "            imgs = imgs.to(device)\n",
    "            features = model(imgs)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "    return np.vstack(all_features)\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(query_features, gallery_features):\n",
    "    query_norm = query_features / (np.linalg.norm(query_features, axis=1, keepdims=True) + 1e-8)\n",
    "    gallery_norm = gallery_features / (np.linalg.norm(gallery_features, axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    similarity_matrix = np.dot(query_norm, gallery_norm.T)\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def evaluate(model, gallery_set, query_set, device, batch_size=16):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    gallery_features = extract_features(model, gallery_set, device, batch_size)\n",
    "    query_features = extract_features(model, query_set, device, batch_size)\n",
    "\n",
    "    similarity_matrix = compute_cosine_similarity(query_features, gallery_features) \n",
    "\n",
    "    query_labels = np.array(query_set.labels_string)\n",
    "    gallery_labels = np.array(gallery_set.labels_string)\n",
    "\n",
    "    topk_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1][:, :5] \n",
    "\n",
    "    rank1_acc = 0\n",
    "    rank5_acc = 0\n",
    "    \n",
    "    for i, q_label in enumerate(query_labels):\n",
    "        retrieved_labels = gallery_labels[topk_indices[i]]\n",
    "        \n",
    "        if retrieved_labels[0] == q_label:\n",
    "            rank1_acc += 1\n",
    "            \n",
    "        if q_label in retrieved_labels:\n",
    "            rank5_acc += 1\n",
    "\n",
    "    rank1_acc = (rank1_acc / len(query_labels))\n",
    "    rank5_acc = (rank5_acc / len(query_labels))\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "    \n",
    "    return rank1_acc, rank5_acc\n",
    "\n",
    "\n",
    "def clean_path(p):\n",
    "    if 'turtles-data/data/' in p:\n",
    "        return p.replace('turtles-data/data/', '')\n",
    "    return p\n",
    "\n",
    "\n",
    "def safe_split(df, name, seed):\n",
    "    if len(df) == 0:\n",
    "        print(f\"WARNING: {name} dataframe is empty!\")\n",
    "        return df, df \n",
    "        \n",
    "    splitter = ClosedSetSplit(ratio_train=0.5, seed=seed)\n",
    "    # Pass values directly to avoid index confusion\n",
    "    splits = splitter.split(df)\n",
    "    \n",
    "    if len(splits) == 0:\n",
    "         print(f\"WARNING: Splitter returned no splits for {name}\")\n",
    "         return df, df\n",
    "\n",
    "    gallery_idx, query_idx = splits[0]\n",
    "    \n",
    "    # Verify indices are valid\n",
    "    if gallery_idx.max() >= len(df) or query_idx.max() >= len(df):\n",
    "         raise IndexError(f\"Splitter returned invalid indices for {name}. Max idx: {gallery_idx.max()}, DF len: {len(df)}\")\n",
    "\n",
    "    gal_df = df.iloc[gallery_idx].reset_index(drop=True)\n",
    "    qry_df = df.iloc[query_idx].reset_index(drop=True)\n",
    "    return gal_df, qry_df\n",
    "\n",
    "\n",
    "def generate_query_gallery_splits(df, seed):\n",
    "    gallery_df, query_df = safe_split(df, \"Dataset Split\", seed)\n",
    "    t_eval = T.Compose([\n",
    "        T.Resize((config['image_size'], config['image_size'])),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    gallery_set = ImageDataset(\n",
    "        gallery_df,\n",
    "        transform=t_eval,\n",
    "        col_path='path',\n",
    "        col_label='identity'\n",
    "    )\n",
    "    query_set = ImageDataset(\n",
    "        query_df,\n",
    "        transform=t_eval,\n",
    "        col_path='path',\n",
    "        col_label='identity'\n",
    "    )\n",
    "    return gallery_set, query_set\n",
    "\n",
    "\n",
    "def partition_data(df, num_clients, seed, overlap_ratio=0.1, max_client_ratio=0.4):\n",
    "    all_identities = sorted(df['identity'].unique().tolist())\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(all_identities)\n",
    "\n",
    "    # 1. Split Identities into Public (Shared) and Private\n",
    "    num_shared = int(len(all_identities) * overlap_ratio)\n",
    "    shared_identities = all_identities[:num_shared]\n",
    "    private_identities = all_identities[num_shared:]\n",
    "\n",
    "    print(f\"Total Identities: {len(all_identities)} | Shared: {len(shared_identities)} | Private: {len(private_identities)}\")\n",
    "\n",
    "    # This map tracks which clients get which IDENTITY (Logic Map)\n",
    "    identity_to_clients_map = {} \n",
    "    \n",
    "    # This map tracks the actual IMAGE INDICES per client (Data Map)\n",
    "    client_image_indices = {i: [] for i in range(num_clients)}\n",
    "\n",
    "    # Max clients a shared identity can belong to\n",
    "    max_clients_limit = max(2, int(num_clients * max_client_ratio))\n",
    "\n",
    "    # 3. Assign Shared Identities (Multi-client)\n",
    "    for identity in shared_identities: # FIXED: Loop over shared, not private\n",
    "        n_partners = rng.randint(2, max_clients_limit + 1)\n",
    "        assigned_clients = rng.choice(num_clients, size=n_partners, replace=False)\n",
    "        identity_to_clients_map[identity] = assigned_clients\n",
    "\n",
    "    # 4. Assign Private Identities (Single-client)\n",
    "    for identity in private_identities:\n",
    "        assigned_client = rng.randint(0, num_clients)\n",
    "        # Store as a list of 1 so the logic below is consistent\n",
    "        identity_to_clients_map[identity] = [assigned_client]\n",
    "\n",
    "    # 5. Pre-shuffle images\n",
    "    id_to_indices = {id: df[df['identity'] == id].index.tolist() for id in all_identities}\n",
    "    for id in id_to_indices:\n",
    "        rng.shuffle(id_to_indices[id])\n",
    "\n",
    "    # 6. Distribute Images\n",
    "    for identity in all_identities:\n",
    "        assigned_clients = identity_to_clients_map[identity]\n",
    "        all_imgs = id_to_indices[identity]\n",
    "        \n",
    "        # 5. Calculate split size\n",
    "        total_shares = len(assigned_clients)\n",
    "        imgs_per_client = len(all_imgs) // total_shares\n",
    "        \n",
    "        for i, client_id in enumerate(assigned_clients):\n",
    "            start = i * imgs_per_client\n",
    "            # If last client, take all remaining to handle odd divisions\n",
    "            if i == total_shares - 1:\n",
    "                end = len(all_imgs)\n",
    "            else:\n",
    "                end = (i + 1) * imgs_per_client\n",
    "                \n",
    "            # Add these specific image rows to the client's pile\n",
    "            if end > start:\n",
    "                client_image_indices[client_id].extend(all_imgs[start:end])\n",
    "\n",
    "    # 7. Build Final DataFrames\n",
    "    client_dfs = []\n",
    "    for client_id in range(num_clients):\n",
    "        indices = client_image_indices[client_id]\n",
    "        # Use .iloc to fetch rows by integer index\n",
    "        client_df = df.loc[indices].copy().reset_index(drop=True)\n",
    "        \n",
    "        # Optional: Add metadata for debugging\n",
    "        client_df['is_shared'] = client_df['identity'].isin(shared_identities)\n",
    "        \n",
    "        client_dfs.append(client_df)\n",
    "    \n",
    "    return client_dfs\n",
    "\n",
    "\n",
    "def move_to_device(obj, device):\n",
    "    \"\"\"Recursively move tensors in a nested structure to the device.\"\"\"\n",
    "    if torch.is_tensor(obj):\n",
    "        return obj.to(device)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: move_to_device(v, device) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [move_to_device(v, device) for v in obj]\n",
    "    elif hasattr(obj, 'to'):  # Handle models/modules\n",
    "        return obj.to(device)\n",
    "    return obj\n",
    "\n",
    "def optimizer_to(optim, device):\n",
    "    \"\"\"Moves optimizer state (momentum, variance) to the specified device.\"\"\"\n",
    "    for state in optim.state.values():\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v):\n",
    "                state[k] = v.to(device)\n",
    "\n",
    "\n",
    "class FederatedClient:\n",
    "    def __init__(self, client_id, train_df, config):\n",
    "        self.client_id = client_id\n",
    "        self.train_df = train_df.copy()\n",
    "        self.config = config\n",
    "        self.device = config['device']  # Target device (GPU)\n",
    "        self.cpu_device = torch.device('cpu')\n",
    "\n",
    "        self.unique_identities = sorted(self.train_df['identity'].unique().tolist())\n",
    "        self.num_local_classes = len(self.unique_identities)\n",
    "\n",
    "        # 1. Initialize on CPU to save memory\n",
    "        backbone = ConvNeXtBackbone(embedding_dim=config['embedding_dim'], pretrained=False)\n",
    "        head = AdaFaceHead(embedding_size=config['embedding_dim'], num_classes=self.num_local_classes)\n",
    "        \n",
    "        # Keep model on CPU\n",
    "        self.model = ReIDModel(backbone, head).to(self.cpu_device)\n",
    "\n",
    "        # Optimizer on CPU parameters\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config['lr'],\n",
    "            weight_decay=self.config['w_decay']\n",
    "        )\n",
    "        self.scaler = grad_scaler.GradScaler()\n",
    "\n",
    "        # Buffers on CPU\n",
    "        self.proto_sums = torch.zeros(self.num_local_classes, config['embedding_dim'], device=self.cpu_device)\n",
    "        self.proto_counts = torch.zeros(self.num_local_classes, device=self.cpu_device)\n",
    "\n",
    "        self.identity_to_local_idx = {id_str: idx for idx, id_str in enumerate(self.unique_identities)}\n",
    "        self.local_idx_to_identity = {idx: id_str for id_str, idx in self.identity_to_local_idx.items()}\n",
    "\n",
    "        print(f\"Client {self.client_id} - Model initialized (CPU-Resident).\")\n",
    "\n",
    "    def get_loader(self):\n",
    "        t_train = T.Compose([\n",
    "            T.Resize((self.config['image_size'], self.config['image_size'])),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "            T.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        dataset = ImageDataset(\n",
    "            self.train_df,\n",
    "            root=self.config['root'],\n",
    "            transform=t_train,\n",
    "            col_path='path',\n",
    "            col_label='identity'\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.config['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def train(self, server_msg):\n",
    "        # --- A. MOVE TO GPU (Active Phase) ---\n",
    "        self.model.to(self.device)\n",
    "        optimizer_to(self.optimizer, self.device)\n",
    "        self.proto_sums = self.proto_sums.to(self.device)\n",
    "        self.proto_counts = self.proto_counts.to(self.device)\n",
    "        \n",
    "        try:\n",
    "            # 1. Load Global Backbone\n",
    "            global_weights = server_msg['model_state']\n",
    "            self.model.backbone.load_state_dict(global_weights, strict=True)\n",
    "            self.model.train()\n",
    "\n",
    "            # 2. Update LR\n",
    "            current_lr = server_msg['current_lr']\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # 3. Prepare Prototypes\n",
    "            global_prototypes = server_msg['prototypes']\n",
    "            target_protos = torch.zeros(self.num_local_classes, self.config['embedding_dim'], device=self.device)\n",
    "            has_proto_mask = torch.zeros(self.num_local_classes, dtype=torch.bool, device=self.device)\n",
    "\n",
    "            if global_prototypes:\n",
    "                for local_idx, identity in self.local_idx_to_identity.items():\n",
    "                    if identity in global_prototypes:\n",
    "                        target_protos[local_idx] = global_prototypes[identity].to(self.device)\n",
    "                        has_proto_mask[local_idx] = True\n",
    "\n",
    "            loader = self.get_loader()\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            # 4. Training Loop\n",
    "            for epoch in range(self.config['local_epochs']):\n",
    "                batch_loss = 0.0\n",
    "                pbar = tqdm(loader, desc=f\"Client {self.client_id} Epoch {epoch+1}\")\n",
    "                \n",
    "                for imgs, labels in pbar:\n",
    "                    imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                    with autocast_mode.autocast(device_type='cuda'):\n",
    "                        logits, emb = self.model(imgs, labels)\n",
    "                        loss_cls = criterion(logits, labels)\n",
    "\n",
    "                        loss_proto = torch.tensor(0.0, device=self.device)\n",
    "                        if self.config['lambda_proto'] > 0 and has_proto_mask.any():\n",
    "                            active_mask = has_proto_mask[labels]\n",
    "                            if active_mask.any():\n",
    "                                active_embs = emb[active_mask]\n",
    "                                active_targets = target_protos[labels[active_mask]]\n",
    "                                loss_proto = F.mse_loss(active_embs, active_targets)\n",
    "                        \n",
    "                        loss_total = loss_cls + (loss_proto * self.config['lambda_proto'])\n",
    "                \n",
    "                    self.optimizer.zero_grad(set_to_none=True)\n",
    "                    self.scaler.scale(loss_total).backward()\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "\n",
    "                    batch_loss += loss_total.item() * imgs.size(0)\n",
    "                    pbar.set_postfix({'loss': f\"{loss_total.item():.4f}\"})\n",
    "            \n",
    "                avg_loss = batch_loss / len(loader.dataset)\n",
    "                \n",
    "            # 5. Compute New Prototypes (Still on GPU)\n",
    "            # Clear cache before inference to make room\n",
    "            torch.cuda.empty_cache()\n",
    "            new_prototypes = self._compute_local_prototypes(loader)\n",
    "            \n",
    "        finally:\n",
    "            # --- B. MOVE BACK TO CPU (Inactive Phase) ---\n",
    "            # Even if training crashes, we must offload to clear GPU for next client\n",
    "            self.model.to(self.cpu_device)\n",
    "            optimizer_to(self.optimizer, self.cpu_device)\n",
    "            self.proto_sums = self.proto_sums.to(self.cpu_device)\n",
    "            self.proto_counts = self.proto_counts.to(self.cpu_device)\n",
    "            \n",
    "            # Clean up GPU tensors\n",
    "            del target_protos, has_proto_mask\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return {\n",
    "            'client_id': self.client_id,\n",
    "            'model_state': self.model.backbone.state_dict(),\n",
    "            'prototypes': new_prototypes,\n",
    "            'num_samples': len(self.train_df),\n",
    "            'loss': avg_loss\n",
    "        }\n",
    "\n",
    "    def _compute_local_prototypes(self, loader):\n",
    "        self.model.eval()\n",
    "        self.proto_sums.zero_()\n",
    "        self.proto_counts.zero_()\n",
    "\n",
    "        dataset_labels = loader.dataset.labels_string\n",
    "        unique_labels = sorted(list(set(dataset_labels)))\n",
    "        int_to_str = {i: s for i, s in enumerate(unique_labels)}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                # Ensure data is on GPU for inference\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Model is already on GPU from train() call\n",
    "                emb = self.model.backbone(images)[0] \n",
    "                \n",
    "                # Accumulate\n",
    "                self.proto_sums.index_add_(0, labels, emb)\n",
    "                self.proto_counts.index_add_(0, labels, torch.ones_like(labels, dtype=torch.float))\n",
    "        \n",
    "        # Calculate averages on CPU to save GPU memory/time\n",
    "        proto_dict = {}\n",
    "        cpu_sums = self.proto_sums.cpu()\n",
    "        cpu_counts = self.proto_counts.cpu()\n",
    "\n",
    "        for idx, count in enumerate(cpu_counts):\n",
    "            if count > 0:\n",
    "                mean_emb = cpu_sums[idx] / count\n",
    "                mean_emb = F.normalize(mean_emb, p=2, dim=0)\n",
    "\n",
    "                identity_str = int_to_str[idx]\n",
    "                proto_dict[identity_str] = (mean_emb, count.item())\n",
    "        \n",
    "        # Restore buffers to correct device (GPU) so 'finally' block can move them to CPU correctly\n",
    "        # (Technically they are already on GPU, but we moved them to CPU for calc)\n",
    "        # Actually, self.proto_sums is still on GPU, we just took a .cpu() copy.\n",
    "        \n",
    "        return proto_dict\n",
    "\n",
    "\n",
    "class FederatedServer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = config['device']\n",
    "        self.global_backbone = ConvNeXtBackbone(embedding_dim=config['embedding_dim']).to(self.device)\n",
    "        self.global_prototypes = {}\n",
    "        self.current_lr = config['lr']\n",
    "\n",
    "        self.dummy_optimizer = optim.AdamW(self.global_backbone.parameters(), lr=config['lr'], weight_decay=config['w_decay'])\n",
    "        warmup_rounds = config['warmup_rounds']\n",
    "        main_scheduler = CosineAnnealingLR(self.dummy_optimizer, T_max=config['rounds'] - warmup_rounds, eta_min=1e-6)\n",
    "        warmup_scheduler = LinearLR(self.dummy_optimizer, start_factor=0.1, total_iters=warmup_rounds)\n",
    "        self.scheduler = SequentialLR(self.dummy_optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_rounds])\n",
    "\n",
    "    def step_scheduler(self):\n",
    "        self.scheduler.step()\n",
    "        self.current_lr = self.scheduler.get_last_lr()[0]\n",
    "        print(f\"[Server] Learning Rate updated to: {self.current_lr:.6f}\")\n",
    "\n",
    "    def aggregate(self, client_msgs):\n",
    "        print(\"[Server] Aggregating Weights and Prototypes...\")\n",
    "        \n",
    "        total_samples = sum(msg['num_samples'] for msg in client_msgs)\n",
    "        first_state = client_msgs[0]['model_state']\n",
    "\n",
    "        agg_state = {k: torch.zeros_like(v) for k, v in first_state.items()}\n",
    "\n",
    "        for msg in client_msgs:\n",
    "            weight_factor = msg['num_samples'] / total_samples\n",
    "            for k, v in msg['model_state'].items():\n",
    "                agg_state[k] += v * weight_factor\n",
    "        \n",
    "        self.global_backbone.load_state_dict(agg_state)\n",
    "\n",
    "        round_sums = {}\n",
    "        round_counts = {}\n",
    "\n",
    "        for msg in client_msgs:\n",
    "            for id_str, (vec, count) in msg['prototypes'].items():\n",
    "                if id_str not in round_sums:\n",
    "                    round_sums[id_str] = vec.float() * count\n",
    "                    round_counts[id_str] = count\n",
    "                else:\n",
    "                    round_sums[id_str] += vec.float() * count\n",
    "                    round_counts[id_str] += count\n",
    "\n",
    "        momentum = self.config['proto_momentum']\n",
    "        updated_cnt = 0\n",
    "        for id_str, vec_sum in round_sums.items():\n",
    "            new_proto = F.normalize(vec_sum / round_counts[id_str], p=2, dim=0)\n",
    "            if id_str in self.global_prototypes:\n",
    "                old_proto = self.global_prototypes[id_str].cpu() # Keep on CPU for storage\n",
    "                avg_proto = (old_proto * momentum) + (new_proto * (1 - momentum))\n",
    "                self.global_prototypes[id_str] = F.normalize(avg_proto, p=2, dim=0)\n",
    "                updated_cnt += 1\n",
    "            else:\n",
    "                self.global_prototypes[id_str] = new_proto\n",
    "\n",
    "        print(f\"[Server] Global Prototypes Updated: {updated_cnt} | Total: {len(self.global_prototypes)}\")\n",
    "\n",
    "    def get_eval_model(self):\n",
    "        eval_model = ReIDModel(self.global_backbone, nn.Identity())\n",
    "        return eval_model\n",
    "\n",
    "    def distribute(self):\n",
    "        comm_msg = {\n",
    "            'model_state': self.global_backbone.state_dict(),\n",
    "            'prototypes': self.global_prototypes,\n",
    "            'current_lr': self.current_lr\n",
    "        }\n",
    "        return comm_msg\n",
    "\n",
    "\n",
    "def main(config):\n",
    "\n",
    "    print(\"Configuration:\")\n",
    "    print(\"-\" * 60)\n",
    "    for key, value in config.items():\n",
    "        print(f\"{key:15s}: {value}\")\n",
    "    print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "    results_path = Path(config['results_root']) / config['results_name']\n",
    "    results_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"--- Loading Data ---\")\n",
    "    SeaTurtleID2022.get_data(root=config['root'])\n",
    "\n",
    "    if config['body_part'] is None:\n",
    "        dataset_df = SeaTurtleID2022(root=config['root']).df\n",
    "    else:\n",
    "        dataset_df = SeaTurtleID2022(root=config['root'], category_name=config['body_part'], img_load='bbox').df\n",
    "\n",
    "    print(f\"Original Dataset Size: {len(dataset_df)}\")\n",
    "\n",
    "    # 2. Load Metadata\n",
    "    try:\n",
    "        # Check standard path\n",
    "        meta_path = Path(config['root']) / 'turtles-data' / 'data' / 'metadata_splits.csv'\n",
    "        meta_df = pd.read_csv(meta_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Metadata not found at standard path.\")\n",
    "        meta_df = None\n",
    "\n",
    "\n",
    "    if meta_df is None:\n",
    "        # Robust fallback search\n",
    "        found_metas = list(Path(config['root']).rglob('metadata_splits.csv'))\n",
    "        if found_metas:\n",
    "            meta_path = found_metas[0]\n",
    "            print(f\"[System] Found metadata at: {meta_path}\")\n",
    "            meta_df = pd.read_csv(meta_path)\n",
    "\n",
    "    \n",
    "    if meta_df is not None:\n",
    "        print(f\"[System] Loading metadata from: {meta_path}\")\n",
    "        dataset_df['join_key'] = dataset_df['path'].apply(clean_path)\n",
    "        \n",
    "        merged_df = pd.merge(\n",
    "            dataset_df, \n",
    "            meta_df[['file_name', 'split_closed', 'split_open']], \n",
    "            left_on='join_key', \n",
    "            right_on='file_name',\n",
    "            how='inner'\n",
    "        )\n",
    "        img_root = 'turtles-data/data/'\n",
    "        merged_df['path'] = merged_df['join_key'].apply(lambda x: str(Path(img_root) / x))\n",
    "    else:\n",
    "        print(\"[WARNING] Metadata not found! Proceeding with raw dataset (splitting might fail).\")\n",
    "        merged_df = dataset_df\n",
    "\n",
    "    print(f\"Merged Dataset Size: {len(merged_df)}\")\n",
    "\n",
    "    display(merged_df.head())\n",
    "\n",
    "    test_path = Path(config['root']) / merged_df.iloc[0]['path']\n",
    "    if not test_path.exists():\n",
    "        print(f\"\\n[FATAL ERROR] Path Verification Failed!\")\n",
    "        print(f\"Root: {config['root']}\")\n",
    "        print(f\"DF Path: {merged_df.iloc[0]['path']}\")\n",
    "        print(f\"Combined: {test_path}\")\n",
    "        raise FileNotFoundError(\"Check dataset structure.\")\n",
    "    print(\"[System]  Path Verification Successful.\")\n",
    "\n",
    "    # 4. Create Base Splits\n",
    "    # split_col = f'split_{config[\"set\"]}'\n",
    "    # train_df = merged_df[merged_df[split_col] == 'train'].reset_index(drop=True)\n",
    "    # valid_df = merged_df[merged_df[split_col] == 'valid'].reset_index(drop=True)\n",
    "    # test_df = merged_df[merged_df[split_col] == 'test'].reset_index(drop=True)\n",
    "\n",
    "    # print(f\"Train: {len(train_df)}, Val: {len(valid_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "    # client_dfs = partition_data(\n",
    "    #     train_df,\n",
    "    #     num_clients=config['num_clients'],\n",
    "    #     seed=config['seed'],\n",
    "    #     overlap_ratio=config['overlap_ratio'],\n",
    "    #     max_client_ratio=config['max_client_ratio']\n",
    "    # )\n",
    "\n",
    "    # print(\"\\n--- Partition Verification ---\")\n",
    "    # for i, df in enumerate(client_dfs):\n",
    "    #     n_unique = df['identity'].nunique()\n",
    "    #     n_shared = df[df['is_shared'] == True]['identity'].nunique()\n",
    "    #     print(f\"Client {i}: {len(df)} images | {n_unique} IDs | {n_shared} Shared IDs\")\n",
    "\n",
    "\n",
    "    # # Generate Query-Gallery splits for validation and testing\n",
    "    # val_gallery_set, val_query_set = generate_query_gallery_splits(valid_df, seed=config['seed'])\n",
    "    # test_gallery_set, test_query_set = generate_query_gallery_splits(test_df, seed=config['seed'])\n",
    "\n",
    "    # server = FederatedServer(config)\n",
    "    # initial_state = server.distribute()['model_state']\n",
    "    \n",
    "    # clients = []\n",
    "    # for i in range(config['num_clients']):\n",
    "    #     client = FederatedClient(i, client_dfs[i], config)\n",
    "    #     client.model.backbone.load_state_dict(initial_state, strict=True)\n",
    "    #     clients.append(client)\n",
    "\n",
    "    # print(\"[Main] Starting Federated Training...\")\n",
    "    # best_val_rank1 = 0.0\n",
    "    # history = {\n",
    "    #     'rank1': [],\n",
    "    #     **{f'loss_C{client_id}': [] for client_id in range(config['num_clients'])}\n",
    "    # }\n",
    "\n",
    "    # early_stopping_counter = 0\n",
    "    # best_round = 0\n",
    "\n",
    "    # for round_idx in range(1, config['rounds'] + 1):\n",
    "    #     print(f\"\\n=== Communication Round {round_idx} / {config['rounds']} ===\")\n",
    "    #     server_msg = server.distribute()\n",
    "\n",
    "    #     client_results = []\n",
    "    #     for client in clients:\n",
    "    #         results = client.train(server_msg)\n",
    "    #         client_results.append(results)\n",
    "    #         history[f'loss_C{client.client_id}'].append(results['loss'])\n",
    "    #         print(f\"   Client {client.client_id} Loss: {results['loss']:.4f}\")\n",
    "\n",
    "    #     server.aggregate(client_results)\n",
    "    #     server.step_scheduler()\n",
    "\n",
    "    #     print(\"[Server] Evaluating Global Model on Validation Set...\")\n",
    "    #     eval_model = server.get_eval_model().to(config['device'])\n",
    "    #     val_r1, val_r5 = evaluate(\n",
    "    #         eval_model, \n",
    "    #         val_gallery_set, \n",
    "    #         val_query_set, \n",
    "    #         config['device'], \n",
    "    #         batch_size=config['batch_size']\n",
    "    #     )\n",
    "    #     print(f\"   Validation Rank-1: {val_r1*100:.2f}%, Rank-5: {val_r5*100:.2f}%\")\n",
    "    #     history['rank1'].append(val_r1)\n",
    "\n",
    "    #     if val_r1 > best_val_rank1:\n",
    "    #         best_val_rank1 = val_r1\n",
    "    #         best_round = round_idx\n",
    "    #         early_stopping_counter = 0\n",
    "    #         torch.save(\n",
    "    #             server.global_backbone.state_dict(),\n",
    "    #             results_path / 'best_backbone.pth'\n",
    "    #         )\n",
    "    #         print(f\"[Server]  New best model found! Evaluating on Test Set... at round {round_idx} with Rank-1: {best_val_rank1*100:.2f}%\")\n",
    "    #     else:\n",
    "    #         early_stopping_counter += 1\n",
    "    #         print(f\"[Server] No improvement. Early Stopping Counter: {early_stopping_counter}/{config['patience']}\")\n",
    "    #         if early_stopping_counter >= config['patience']:\n",
    "    #             print(\"[Server] Early stopping triggered. Ending training.\")\n",
    "    #             break\n",
    "    \n",
    "    # with open(results_path / 'training_history.json', 'w') as f:\n",
    "    #     json.dump(history, f, indent=4)\n",
    "\n",
    "    # print(f\"\\n=== Training Complete. Best Validation Rank-1: {best_val_rank1*100:.2f}% at round {best_round} ===\")\n",
    "    # print(\"[Server] Loading Best Model for Final Evaluation...\")\n",
    "    # best_state = torch.load(results_path / 'best_backbone.pth')\n",
    "    # server.global_backbone.load_state_dict(best_state)\n",
    "    # final_model = server.get_eval_model().to(config['device'])\n",
    "    # test_r1, test_r5 = evaluate(\n",
    "    #     final_model, \n",
    "    #     test_gallery_set, \n",
    "    #     test_query_set, \n",
    "    #     config['device'], \n",
    "    #     batch_size=config['batch_size']\n",
    "    # )\n",
    "    # print(f\"[Server] Final Test Set Performance - Rank-1: {test_r1*100:.2f}%, Rank-5: {test_r5*100:.2f}%\")\n",
    "\n",
    "    # with open(results_path / 'results.txt', 'w') as f:\n",
    "    #     f.write(f\"Best Validation Rank-1 Accuracy: {best_val_rank1*100:.2f}% at round {best_round}\\n\")\n",
    "    #     f.write(f\"Test Rank-1 Accuracy: {test_r1*100:.2f}%, Rank-5 Accuracy: {test_r5*100:.2f}%\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # config = {\n",
    "    #     'root': './data/SeaTurtleID2022',\n",
    "    #     'results_root': './results/federated_reid', \n",
    "    #     'description': '',\n",
    "    #     'image_size': 384,\n",
    "    #     'batch_size': 128,\n",
    "    #     'patience': 8,\n",
    "\n",
    "    #     'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    #     'seed': 42,\n",
    "    #     'body_part': 'head',  # 'head', 'turtle', 'flipper', or None for full image\n",
    "    #     'set': 'closed',      # 'closed' or 'open'\n",
    "    #     'lr': 1e-4,\n",
    "    #     'w_decay': 1e-4,\n",
    "\n",
    "    #     'embedding_dim': 512,\n",
    "        \n",
    "    #     # Federated Settings\n",
    "    #     'num_clients': 5,\n",
    "    #     'overlap_ratio': 0.1,\n",
    "    #     'max_client_ratio': 0.4,\n",
    "    #     'local_epochs': 1,\n",
    "    #     'rounds': 30,\n",
    "    #     'lambda_proto': 0.1,\n",
    "    #     'proto_momentum': 0.9,\n",
    "    #     'warmup_rounds': 5,\n",
    "    # }\n",
    "\n",
    "    # experiments = [\n",
    "    #     {\n",
    "    #         'results_name': 'TEST_RUN',\n",
    "    #         'description': 'Test run with default settings',\n",
    "    #         'body_part': 'head',\n",
    "    #         'set': 'closed',\n",
    "    #         'seeds': [42],\n",
    "    #     },\n",
    "    # ]\n",
    "\n",
    "    # for exp in experiments:\n",
    "    #     print(f\"Starting {exp['results_name']}...\")\n",
    "    #     for seed in exp['seeds']:\n",
    "    #         exp_config = config.copy()\n",
    "    #         exp_config.update(exp)\n",
    "    #         exp_config['results_name'] = f\"{exp['results_name']}_SEED_{seed}\"\n",
    "    #         exp_config['seed'] = seed\n",
    "    #         exp_config.pop('seeds', None)\n",
    "    #         main(exp_config)\n",
    "    convnext = ConvNeXtBackbone()\n",
    "    print(convnext)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
